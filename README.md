# PyTorch Distillation
This package consists of a small extension library for Knowledge Distillation in PyTorch.

## Installation
### From source
Pytorch Distillation requires PyTorch (>= 1.4.0) to be installed. Please refer to the [PyTorch installation page](https://pytorch.org/get-started/locally/#start-locally) regarding the specific install command for your platform.
```bash
git clone https://github.com/sgraaf/pytorch_distillation.git
cd pytorch_distillation
pip setup.py install
```

## Example
Please refer to the [examples](https://github.com/sgraaf/pytorch_distillation/tree/master/examples) in the `examples/` directory for some working example(s) of Knowledge Distillation.

### Acknowledgements
PyTorch Distillation has adapted (parts of) some example code of the [Transformers library](https://github.com/huggingface/transformers) by ðŸ¤— Hugging Face, Inc.

### License
PyTorch Distillation is open-source and licensed under GNU GPL, Version 3.
